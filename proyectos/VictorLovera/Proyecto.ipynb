{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto de algoritmo de análisis de sentimiento difuso a partir de tweets.\n",
    "\n",
    "### Autor: Victor Ulises Lovera Duarte\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1:\n",
    "Para el primer paso tenemos que leer y hacer un preprocesado de los datos. Para la lectura usamos la librería de **pandas** que, no solo tiene una función para extraer los datos directamente del csv, si no que también es una herramienta que nos facilitara el procesamiento de los datos y su posterior exposición.\n",
    "Luego pasamos al preprocesado de los datos para estos usamos la librería de **re** que nos ayuda a quitar los componentes de texto que no queremos, además de remplazar alguna de las palabras acortadas comúnmente usadas, por ultimo pasamos todo a minúsculas, esto hacemos por cada oración en los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def Leer_Archivo(Archivo):\n",
    "    data = pd.read_csv(Archivo)\n",
    "    data.insert(0, 'Id', range(1, 1 + len(data)))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def Pre_proceso_Texto(data):\n",
    "    def process(text):\n",
    "        \n",
    "        text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "        text = re.sub(r' www\\S+', '', text)\n",
    "        text = re.sub(r'@\\S+', '', text)\n",
    "        text = re.sub(r'[^\\w\\s]|[\\d]', ' ', text)\n",
    "        text = re.sub(r'\\s\\s+', ' ', text)\n",
    "        text = re.sub(r\"#\", \"\", text)\n",
    "        \n",
    "        text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'re\", \" are\", text)\n",
    "        text = re.sub(r\"\\'s\", \" is\", text)\n",
    "        text = re.sub(r\"\\'d\", \" would\", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "        text = re.sub(r\"\\'t\", \" not\", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "        text = re.sub(r\"\\'m\", \" am\", text)\n",
    "        \n",
    "        text = text.strip().lower().encode('ascii', 'ignore').decode()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        row['sentence'] = process(row['sentence'])\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2:\n",
    "Ahora debemos de realizar el análisis del Lexicón de Sentimientos, para esto usamos una librería llamada **NLTK** que nos provee con las funciones necesarias para lograrlo. Llegados a este punto también comenzamos calculando el tiempo que nos lleva analizar el Lexicón de Sentimientos de cada oración y los incluimos como una columna más en los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "def Lexicon_Sentiminetos(data):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    res = {}\n",
    "    tiempos = []\n",
    "    for i, row in data.iterrows():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        text = row['sentence']\n",
    "        myid = row['Id']\n",
    "        res[myid] = sia.polarity_scores(text)\n",
    "        \n",
    "        tiempos.append(time.time() - start_time)\n",
    "\n",
    "    Datos_Sent = pd.DataFrame(res).T\n",
    "    Datos_Sent = Datos_Sent.reset_index().rename(columns={'index': 'Id'})\n",
    "    Datos_Sent['Tiempo Computo'] = tiempos\n",
    "    \n",
    "    Datos_Sent = Datos_Sent.merge(data, how='left').drop(['compound', 'neu'], axis=1)\n",
    "    \n",
    "    return Datos_Sent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 3:\n",
    "En este punto empezamos delimitando el Universo de nuestras funcionen difusas, así como también las funciones de membresía de las varias funciones que vamos a usar estas son:\n",
    "1. **Sent_Postivo**: Universo de los resultados positivos del algoritmo anterior\n",
    "   1. **sp_lo**\n",
    "   2. **sp_med**\n",
    "   3. **sp_hi**\n",
    "2. **Sent_Negativo**: Universo de los resultados negativos del algoritmo anterior\n",
    "   1. **sn_lo**\n",
    "   2. **sn_med**\n",
    "   3. **sn_hi**\n",
    "3. **Out_U**: Universo de la salida de las reglas que serán aplicados\n",
    "   1. **out_neg**\n",
    "   2. **out_neu**\n",
    "   3. **out_pos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "Sent_Postivo = np.arange(0, 1, 0.01)\n",
    "Sent_Negativo = np.arange(0, 1, 0.01)\n",
    "Out_U = np.arange(0, 10, 1)\n",
    "\n",
    "sp_lo = fuzz.trimf(Sent_Postivo, [0, 0, 0.5])\n",
    "sp_med = fuzz.trimf(Sent_Postivo, [0, 0.5, 1])\n",
    "sp_hi = fuzz.trimf(Sent_Postivo, [0.5, 1, 1])\n",
    "\n",
    "sn_lo = fuzz.trimf(Sent_Negativo, [0, 0, 0.5])\n",
    "sn_med = fuzz.trimf(Sent_Negativo, [0, 0.5, 1])\n",
    "sn_hi = fuzz.trimf(Sent_Negativo, [0.5, 1, 1]) \n",
    "\n",
    "out_neg = fuzz.trimf(Out_U, [0, 0, 5])\n",
    "out_neu = fuzz.trimf(Out_U, [0, 5, 10])\n",
    "out_pos = fuzz.trimf(Out_U, [5, 5, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez optemos los datos del análisis del Lexicón de Sentimientos, más específicos los valores puntajes *positivos* y *negativos* los usamos juntos a las reglas propuestas por [Vashishtha S., & Susan, S. (2019). **Fuzzy rule based unsupervised sentiment analysis from social media posts**](https://www.researchgate.net/profile/Srishti-Vashishtha-2/publication/334622166_Fuzzy_Rule_based_Unsupervised_Sentiment_Analysis_from_Social_Media_Posts/links/5ece42174585152945149e5b/Fuzzy-Rule-based-Unsupervised-Sentiment-Analysis-from-Social-Media-Posts.pdf), y por ultimo utilizamos el método de deFuzzification del Centro de Gravedad para final mente hallar la métrica que nos determina si la oración es positiva o negativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fuzzy_DeFuzzy(scoreposi, scoreneg):\n",
    "    \n",
    "    scoreposi = max(0, min(1, scoreposi))\n",
    "    scoreneg = max(0, min(1, scoreneg))\n",
    "    \n",
    "    pos_n_lo = fuzz.interp_membership(Sent_Postivo, sp_lo, scoreposi)\n",
    "    pos_n_med = fuzz.interp_membership(Sent_Postivo, sp_med, scoreposi)\n",
    "    pos_n_hi = fuzz.interp_membership(Sent_Postivo, sp_hi, scoreposi)\n",
    "\n",
    "    neg_n_lo = fuzz.interp_membership(Sent_Negativo, sn_lo, scoreneg)\n",
    "    neg_n_med = fuzz.interp_membership(Sent_Negativo, sn_med, scoreneg)\n",
    "    neg_n_hi = fuzz.interp_membership(Sent_Negativo, sn_hi, scoreneg)\n",
    "\n",
    "    regla1 = np.fmin(pos_n_lo, neg_n_lo)\n",
    "    regla2 = np.fmin(pos_n_med, neg_n_lo) \n",
    "    regla3 = np.fmin(pos_n_hi, neg_n_lo) \n",
    "    regla4 = np.fmin(pos_n_lo, neg_n_med) \n",
    "    regla5 = np.fmin(pos_n_med, neg_n_med) \n",
    "    regla6 = np.fmin(pos_n_hi, neg_n_med) \n",
    "    regla7 = np.fmin(pos_n_lo, neg_n_hi) \n",
    "    regla8 = np.fmin(pos_n_med, neg_n_hi) \n",
    "    regla9 = np.fmin(pos_n_hi, neg_n_hi)\n",
    "\n",
    "    temp1 = np.fmax(regla4, regla7)\n",
    "    regla_neg = np.fmax(temp1, regla8)\n",
    "    out_act_low = np.fmin(regla_neg, out_neg)\n",
    "\n",
    "    temp2 = np.fmax(regla1, regla5)\n",
    "    regla_neu = np.fmax(temp2, regla9)\n",
    "    out_act_neu = np.fmin(regla_neu, out_neu)\n",
    "\n",
    "    temp3 = np.fmax(regla2, regla3)\n",
    "    regla_pos = np.fmax(temp3, regla6)\n",
    "    out_act_pos = np.fmin(regla_pos, out_pos)\n",
    "\n",
    "    aggregated = np.fmax(out_act_low,\n",
    "                        np.fmax(out_act_neu, out_act_pos))\n",
    "\n",
    "    return fuzz.defuzz(Out_U, aggregated, 'centroid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ya solo queda seguir todos los pasos, con esta función retornamos el DataFrame ya con los resultados y los costes temporales resultante para cada oración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Computar_Datos(Archivo):\n",
    "    \n",
    "    \n",
    "    Datos = Leer_Archivo(Archivo)\n",
    "    \n",
    "    Datos = Pre_proceso_Texto(Datos)\n",
    "    \n",
    "    Datos = Lexicon_Sentiminetos(Datos)\n",
    "    \n",
    "    Resultados = Datos.copy()\n",
    "\n",
    "    tiempos = []\n",
    "    for i, row in Datos.iterrows():\n",
    "        start_time = time.time()\n",
    "        \n",
    "        Resultados.loc[i, 'Results'] = Fuzzy_DeFuzzy(row['pos'], row['neg'])\n",
    "        \n",
    "        tiempos.append(time.time() - start_time)\n",
    "\n",
    "    Resultados['Tiempo Computo'] = Resultados['Tiempo Computo'] + tiempos\n",
    "    \n",
    "    Resultados.drop('Id', axis=1)\n",
    "    Resultados = Resultados[['sentence', 'sentiment', 'pos', 'neg', 'Results', 'Tiempo Computo']]\n",
    "    Resultados.columns = ['Oración', 'Etiqueta original', 'Puntaje Positivo', 'Puntaje Negativo', 'Resultado de inferencia', 'Tiempo Computo']\n",
    "\n",
    "    return Resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Oración</th>\n",
       "      <th>Etiqueta original</th>\n",
       "      <th>Puntaje Positivo</th>\n",
       "      <th>Puntaje Negativo</th>\n",
       "      <th>Resultado de inferencia</th>\n",
       "      <th>Tiempo Computo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i loooooooovvvvvveee my kindle not that the dx...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.371795</td>\n",
       "      <td>0.002509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reading my kindle love it lee childs is good read</td>\n",
       "      <td>1</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.963203</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ok first assesment of the kindle it fucking rocks</td>\n",
       "      <td>1</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.890017</td>\n",
       "      <td>0.000999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you ll love your kindle i ve had mine for a fe...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.135</td>\n",
       "      <td>4.685033</td>\n",
       "      <td>0.001006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fair enough but i have the kindle and i think ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.849136</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Oración  Etiqueta original  \\\n",
       "0  i loooooooovvvvvveee my kindle not that the dx...                  1   \n",
       "1  reading my kindle love it lee childs is good read                  1   \n",
       "2  ok first assesment of the kindle it fucking rocks                  1   \n",
       "3  you ll love your kindle i ve had mine for a fe...                  1   \n",
       "4  fair enough but i have the kindle and i think ...                  1   \n",
       "\n",
       "   Puntaje Positivo  Puntaje Negativo  Resultado de inferencia  Tiempo Computo  \n",
       "0             0.382             0.000                 5.371795        0.002509  \n",
       "1             0.470             0.000                 5.963203        0.000000  \n",
       "2             0.216             0.000                 4.890017        0.000999  \n",
       "3             0.204             0.135                 4.685033        0.001006  \n",
       "4             0.456             0.000                 5.849136        0.000000  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Archivo = './Data/test_data.csv'\n",
    "    \n",
    "Resultados = Computar_Datos(Archivo)\n",
    "\n",
    "Resultados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Promedio temporal de Ejecucion es: 0.00042291604044709697\n"
     ]
    }
   ],
   "source": [
    "print(\"El Promedio temporal de Ejecucion es:\", Resultados['Tiempo Computo'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Total_Pos_Neg(data):\n",
    "    \n",
    "    pos_t = 0\n",
    "    neg_t = 0\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        if row['Resultado de inferencia'] < 5:\n",
    "            neg_t += 1\n",
    "        else:\n",
    "            pos_t += 1\n",
    "        \n",
    "    return pos_t, neg_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "La cantidad de Oraciones Positivas es:  98\n",
      "\n",
      "La cantidad de Oraciones Negativas es:  261\n"
     ]
    }
   ],
   "source": [
    "Total_Positivos, Total_Negativos = Total_Pos_Neg(Resultados)\n",
    "print(\"\\nLa cantidad de Oraciones Positivas es: \", Total_Positivos)\n",
    "print(\"\\nLa cantidad de Oraciones Negativas es: \", Total_Negativos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
